---
title: "regression"
output: 
  html_document:
    pandoc_args:
    - +RTS
    - -K64m
    - -RTS
    toc: yes
---

##learning resources  

* **Data Specialization Course**  
Videos: search Brian Caffo on YouTube  
Code: https://github.com/DataScienceSpecialization/courses and  
Notes: http://datasciencespecialization.github.io/   

* **ISLR Videos and Notes**
https://www.dataschool.io/15-hours-of-expert-machine-learning-videos/ 



## proof of least squares solution
https://math.stackexchange.com/questions/131590/derivation-of-the-formula-for-ordinary-least-squares-linear-regression 

## linear regression using the linear model (lm) function

### making up fake data to work with

```{r comment=NA, message=FALSE}
library(UsingR)
library(ggplot2)

# generate some fake data
set.seed(1234)
beta <- 2
intercept <- 10
n <- 50
m <- 10
s <- 10
noise <- rnorm(n, mean = m, sd = s)
observed <- runif(n = 50, min = 1, max = 100 )
outcome <- beta*observed + rep(intercept, n) + noise
fake <- as.data.frame(cbind(observed, outcome))
```

### basic lm usage and examination of model

```{r comment=NA, message=FALSE}

# plot the data
plot(fake$observed, fake$outcome,  
     xlab = "observed (units)", 
     ylab = "outcome (units)", 
     bg = "lightblue", 
     col = "black", cex = 1.1, pch = 21,frame = FALSE)


# calculate the coefficients of the linear model
fit <- lm(outcome ~ observed, data = fake)


#plot the regression line and the predicted points
abline(fit, lwd = 2)
points(fake$observed, predict(fit), pch = 19, col = "red") 

# examine the coefficients and the generated model
fit
coef(fit)
summary(fit)

# to show the following 6) plots
# 1) a plot of residuals against fitted values (should be uncorrelated with fitted (observed) values), 
# 2) a Scale-Location plot of sqrt(| residuals |) against fitted values, 
# 3) a Normal Q-Q plot (residuals (error terms) are assumed to follow a normal distribution for many tests), 
# 4) a plot of Cook's distances versus row labels, 
# 5) a plot of residuals against leverages, and 
# 6) a plot of Cook's distances against leverage/(1-leverage). 
# see methods(plot) then ?plot.lm for more
plot(fit, which = c(1:6))
# typing in ?influence.measures in R will display the detailed documentation on all available functions to measure influence
# e.g. 
# hatvalues(model) = measures of leverage
# dfbetas(model) = change in individual coefficients when the ith point is deleted in fitting the model - these can be used to evaluate how deleting or including this point impact a particular aspect of the model
# effectively measures influence of the individual coefficients
# cooks(model).distance = overall change in coefficients when the ith point is deleted
# #many others


```

### centering data 

regression on centred data will give intercept that is y value for mean value of x

```{r}

#the I notation is a shortcut to allow evaluation of variable in-line with lm call
#fit2 <- lm(outcome ~ I(observed - mean(observed)), data = fake)

# centre
observed.c <- observed - mean(observed)
fit2 <- lm(outcome ~ observed.c, data = fake)
coef(fit2)
plot(observed.c, fake$outcome,  
     xlab = "observed (units)", 
     ylab = "outcome (units)", 
     bg = "lightblue", 
     col = "black", cex = 1.1, pch = 21,frame = FALSE)
#plot the regression line and the predicted points
abline(fit2, lwd = 2)
points(observed.c, predict(fit2), pch = 19, col = "red") 

#scale
observed.s <- (observed - mean(observed))/sd(observed)
fit3 <- lm(outcome ~ observed.s, data = fake)
coef(fit3)
plot(observed.s, fake$outcome,  
     xlab = "observed (units)", 
     ylab = "outcome (units)", 
     bg = "lightblue", 
     col = "black", cex = 1.1, pch = 21,frame = FALSE)
#plot the regression line and the predicted points
abline(fit3, lwd = 2)
points(observed.s, predict(fit3), pch = 19, col = "red") 


```

### making predictions with a model

```{r}
# x = observations for which we want to make predictions using the generated model
x <- c(10,50,90)
# x must be passed as a column named after the expected predictor variable in a df
predict(fit, newdata = data.frame(observed = x))

```

### plotting regression lines in ggplot

https://www.rstudio.com/wp-content/uploads/2015/03/ggplot2-cheatsheet.pdf 

```{r}
g <- ggplot(fake, aes(x=observed, y=outcome))
g <- g + xlab("observed (units)")
g <- g + ylab("outcome (units)")
g <- g + geom_point(size = 2, colour = "black", alpha = 0.4)
g <- g + geom_point(size = 1, colour = "blue", alpha = 0.2)
g <- g + geom_smooth(method="lm", colour="black")
g

```

### plotting interactive regression plot in plotly

https://plot.ly/ggplot2/geom_abline/  

```{r, message=F, warning=F}
library(plotly)
p <- ggplotly(g)
p

```


### obtain residuals for a fit

```{r}
e <- resid(fit)
#or
fit$residuals
```




```{r name, echo=TRUE, eval=TRUE, results='show', cache=FALSE, comment=NA}

#

```

